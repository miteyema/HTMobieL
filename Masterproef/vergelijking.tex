%TODO criteria bekijken

\chapter{Vergelijkingscriteria}
\label{chap:vergelijkingscriteria}

Dit hoofdstuk bekijkt hoe de mobiele HTML5 raamwerken actief zullen worden vergeleken.
Hoofdzakelijk zal dit gebeuren aan de hand van een \term{proof of concept}~(POC).
Deze wordt geïntroduceerd in sectie \ref{sec:vergelijking-poc} en zal hoofdzakelijk de gekozen vergelijkingscriteria in sectie \ref{sec:vergelijking-criteria} drijven.
De criteria die worden voorgesteld, zullen voortaan actieve criteria worden genoemd.


\section{POC}
\label{sec:vergelijking-poc}
In samenspraak met Capgemini werd gekozen om een \term{proof of concept}~(POC) op te stellen.
Dit is een idee waarbij de uitvoerbaarheid in de verschillende raamwerken kan worden nagegaan.
Verschillende vergaderingen werden georganiseerd om tot een idee te komen dat vooral in de bedrijfswereld van toepassing is.
Het uiteindelijke idee is een applicatie die het mogelijk maakt voor werknemers om hun onkosten via hun mobiel apparaat door te sturen.

Het idee werd uitgewerkt door Capgemini en geleverd aan de auteurs als \term{mockup}.
Dit is een voorstelling van de applicatie als een reeks schermen zoals deze er zouden uitzien op een apparaat. 
Een voorbeeld van zo een scherm is te vinden op figuur~\ref{fig:poc}. 
De bedoeling is dat deze POC wordt uitgewerkt zowel voor smartphone als tablet, zowel voor Android als iOS, zowel voor staande als liggende apparaten en zowel voor online als offline gebruik.

\begin{figure}
  \centering
  \includegraphics[trim=0cm 4cm 0cm 1.25cm,clip=true,height=7.5cm]{figuren/poc.pdf}
  \caption{POC bij het toevoegen van een nieuwe onkost met aan de linkerkant de weergave op een tablet en aan de rechterkant deze op een smartphone.}
  \label{fig:poc}
\end{figure}

\subsection{Aspecten}
\label{sec:vergelijking-poc-detail}

Een werknemer meldt zich eerst aan op de applicatie en kan daarna ofwel een nieuw onkostenformulier aanmaken of zijn doorgestuurde onkostenformulieren bekijken.
De term onkostenformulier is een groepering van meerdere onkosten met bijhorende bewijsstukken en de handtekening van de werknemer. 
Het aanmaken van een nieuw onkostenformulier verloopt in vier stappen.
Indien de werknemer al eerder begonnen was met het aanmaken van een formulier, zal hij worden gevraagd of hij verder wil gaan met dat formulier of met een nieuw formulier wil starten.

\begin{enumerate}
\item De eerste stap is het bekijken en/of aanpassen van de persoonlijke informatie van de werknemer.
Bij het aanpassen van deze gegevens, zullen deze worden gevalideerd.
Indien deze validatie faalt, krijg de werknemer een dialoogvenster te zien met de reden tot falen.
Ook worden de foute velden rood gemarkeerd.

\item In de tweede stap kan de werknemer zijn toegevoegde onkosten aan het formulier bekijken.
In het begin is deze lijst leeg, tenzij hij eerder een formulier aan het invullen was (zie infra).
Indien deze lijst onkosten bevat, is het mogelijk om hierop te klikken en deze te bekijken. Aanpassen is niet mogelijk.

\item In stap drie kan een nieuwe onkost worden toegevoegd.
Dit kan ofwel een binnenlandse ofwel buitenlandse onkost zijn.
Voor beide dient een datum en projectcode te worden opgegeven.
De eerstgenoemde is een \term{datepicker} die teruggaat tot twee maanden in de tijd.
De laatstgenoemde bevat automatische aanvulling, maar de werknemer is niet verplicht om een projectcode uit de aanvulling te selecteren.
Daarnaast dient het type en bedrag van de onkost, alsook een bewijsstuk te worden opgegeven.
Bij een buitenlands onkost moet de munteenheid worden opgegeven, waarna de applicatie deze automatisch omvormt naar euro.
Het scherm voor het toevoegen van een buitenlandse onkost wordt getoond op figuur \ref{fig:poc}. 
Net zoals bij stap één geldt ook hier validatie op de formuliervelden.

\item In deze laatste stap dient een handtekening te worden geplaatst waarna het formulier kan worden doorgestuurd.
Indien de gebruiker offline werkt, zal deze worden opgeslagen op het toestel.
De werknemer kan het formulier opnieuw doorsturen zodra hij terug online is.

\end{enumerate}

Bij het bekijken van de doorgestuurde formulieren is het mogelijk om per formulier de bijhorende PDF te downloaden. 
Deze bevat een overzicht van de onkosten met bijhorende bewijsstukken, alsook de handtekening van de werknemer.

\section{Criteria}
\label{sec:vergelijking-criteria}

In deze sectie zullen de actieve criteria toelicht worden die zullen worden toegepast om de raamwerken te vergelijken.
In sectie \ref{sec:vergelijken-raamwerken} werden reeds technieken besproken die in de literatuur worden toegepast.
Elementen van deze technieken zullen terugkomen in onze methode om de raamwerken te evalueren.
Sectie \ref{sec:raamwerken-tabel} bevatte de passieve vergelijkingscriteria die raamwerken vergeleken met informatie van het raamwerk zelf.

Vijf grote criteria zullen worden gebruikt: populariteit (\ref{sec:vergelijking-populariteit}), productiviteit (\ref{sec:vergelijking-productiviteit}), gebruik (\ref{sec:vergelijking-gebruik}), ondersteuning (\ref{sec:vergelijking-ondersteuning}) en performantie (\ref{sec:vergelijking-performantie}). 
Elk raamwerk krijgt voor elk criterium een score. 
Deze scores zullen in een spinnenweb worden ondergebracht (zie sectie \ref{sec:vergelijking-spinnenweb}).
Zoals hierboven vermeld zal een POC gebruikt worden bij de vergelijking.
De implementatie van deze POC zal het productiviteits-, gebruiks- en ondersteuningscriterium drijven.  
Dit komt omdat Capgemini de POC zo heeft opgesteld dat het verschillende functionaliteit bevat om een volledige vergelijking te kunnen maken.

De score van performantie zal deels bepaald worden door de laadtijden van de POC.

\subsection{Populariteit}
\label{sec:vergelijking-populariteit}
De populariteit van een raamwerk is in cijfers uit te drukken door gebruik te maken van sociale netwerken. 
Een tabel zal voorzien worden met in de kolommen het aantal volgers op Twitter, sterren en forkers van GitHub,  vragen op Stackoverflow en aantal vind-ik-leuks van Facebook en de raamwerken in de rijen~\cite{Sarrafi2012a,Ayuso2012}. 

GitHub kan worden gezien als een sociaal netwerk voor programmeurs~\cite{Catone2008} en bepaalt dus de actieve gemeenschap rond het raamwerk.
%TODO ik zou gewoon algemeen zeggen 'raamwerken die niet op github zijn, krijgen 0'
Zowel \st{} als \kendo{} echter, zijn niet op GitHub terug te vinden.
Hierdoor zal de score voor sterren en forkers van GitHub voor beide raamwerken gelijk zijn aan nul.
Een alternatief hield de interpolatie van de GitHub data van de overige raamwerken in.
Omdat deze aanpak het raamwerk onterecht zou bevoordelen, is hier niet voor gekozen.

De som van Twitter volgers ($T_r$), GitHub sterren ($S_r$), GitHub forkers ($F_r$), Stackoverflow vragen ($SO_r$) en Facebook vind-ik-leuks ($FB_r$) vormt de score voor het gemeenschapscriteria:
\begin{equation}
  \text{Populariteit}_r=T_r+S_r+F_r+SO_r+FB_r
  \label{eq:populariteit}
\end{equation}
met $r$ de verschillende raamwerken.

De uitkomsten van dit criterium zullen vergeleken worden met data geleverd door Google Trends~\cite{Google2012a}.
Deze webapplicatie toont de evolutie van zoektermen op Google op een schaal van 100, waarbij 100 overeenkomt met de grootste zoekinteresse.
Voor elk raamwerk zal het volume zoekopdrachten naar het raamwerk in functie van de tijd worden uitgezet.
%TODO bekijken evolutie van data in de tijd?

  Er bestaat geen exacte formule om populariteit uit te drukken.
De formule die werd gekozen om de score voor dit criterium te quoteren is onderheven aan subjectiviteit.
Twee opmerkingen moeten hierbij worden gemaakt.
Enerzijds zijn de auteurs zich ervan bewust dat de doorsnede tussen sociale netwerken niet leeg is.
Zo kan éénzelfde persoon zowel een volger op Twitter zijn als fan op Facebook.
Verschillende individuen zullen dus dubbel geteld worden in de totale score voor populariteit van het raamwerk.
De score zal dus slechts een indicatie geven over de popularieit,  het is geen exacte weergave.
Ten tweede zijn de auteurs er zicht van bewust dat de inclusie van Stackoverflow op twee manieren kan worden bekeken.
Enerzijds kunnen veel vragen over één onderwerp op Stackoverflow duiden op veel vraagtekens over dit onderwerp.
Anderzijds kan dit een maat zijn voor de populariteit van dit onderwerp.
De auteurs zijn van mening dat de tweede zienswijze correcter is dan de eerste en het dus valied is Stackoverflow in de formule op te nemen.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Productiviteit}
\label{sec:vergelijking-productiviteit}
De productiviteit moet een indicatie geven hoe lang het duurt om met het raamwerk vertrouwd te raken.
De auteurs zullen het raamwerk testen met een implementatie van de POC.
Er kan verondersteld worden dat de auteurs over een gemeenschappelijke technische achtergrond beschikken.

Elk zullen ze de POC in twee verschillende raamwerken maken,  daarnaast ook een extra login applicatie in twee andere raamwerken.
Tim Ameye maakt de POC in \jqm{} en \lungo{} en de login applicatie in \st{} en \kendo{}.
Sander Van Loock zal dan de POC in \st{} en \kendo{} maken en de login applicaties in \jqm{} en \lungo{}.
De tijd die nodig is om de volledige POC te implementeren is een indicatie voor de productiviteit. 
De implementatie van de login applicatie is een extra test.
Deze applicatie bevat GI-elementen, validaties,  \term{backend} integratie en lijsten.
Deze test kan dit dus als voldoende steekproef beschouwd worden om ervaring met een raamwerk te testen.
Na het aanmelden met deze applicatie zal de gebruiker een lijst van $850$ elementen te zien krijgen.
Deze lijst is voornamelijk bedoeld om de performantie te testen (zie sectie \ref{sec:vergelijking-performantie}).
De elementen in de lijst zullen voorzien worden van een afbeelding en tekst.
Het aantal elementen in de lijst - $850$ - is een schatting van het aantal liedjes die Elvis Presley ooit heeft opgenomen~\cite{Zimmy2011}.
Deze keuze werd gemaakt met het oog op potentiele muziek applicaties die alle nummers van een artiest ook in lijsten samenbrengt.
Elvis Presley is de artiest die het meeste opgenomen nummers achter zijn naam heeft staan en kan dus als bovengrens voor dit soort applicaties worden beschouwd.
De implementatie van deze applicatie zal een indicatie geven hoe snel,  zonder al te veel voorkennis van het raamwerk,  één eenvoudige applicatie opgeleverd kan worden.
Omdat de POC twee keer moet worden geïmplementeerd, wordt verwacht dat de tweede implementatie sneller zal verlopen.
Dit probleem is onafwendbaar en zal bij de evalatie van de data aangehaald worden.
De uren voor de implementatie van de loginapplicatie zal de score correcter maken.

De som van de uren voor het implementeren van de POC ($t_{r,POC}$) en de login applicatie ($t_{r,login}$) vormt de score voor de productiviteit:
\begin{equation}
  \text{Productiviteit}_r = {t_{r,POC} + t_{r,login}}
  \label{eq:productiviteit}
\end{equation}
waar $r$ de verschillende raamwerken voorstellen.

De uitkomsten van dit criterium zullen gestaafd worden met enkele argumenten.
Eerst zullen het aantal tools bekeken worden die de programmeur kan gebruiken om eenvoudiger te ontwikkelen.
Ook zullen de factoren die de leercurve bepalen, worden geschat.
Vervolgens zal de kwaliteit en quantiteit van de documentatie van elk raamwerk worden bekekenen.
De mogelijkheden voor debuggen bepalen ook de productiviteit en zullen bekeken worden.
Dan zal gekeken worden naar de aanwezige literatuur van het raamwerk en waar ontwikkelaars met vragen terecht kunnen.
Ten slotte zullen de lijnen code die nodig waren voor zowel de POC als de loginapplicatie gepresenteerd worden.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Gebruik}
\label{sec:vergelijking-gebruik}

Deze POC kan onderverdeeld worden in verschillende deelproblemen.
Deze deelproblemen worden als uitdaging aan het raamwerk voorgelegd.
Alle uitdagingen en deeluitdagingen zijn hieronder te vinden.

%\begin{table}[H]
%\centering
\pgfplotstabletypeset[
  begin table=\begin{longtable}{p{12cm}},
  end table=\end{longtable},
  skip coltypes=true,
  col sep=comma,
  string type,
  header=true,
  columns={Uitdaging},
  columns/Uitdaging/.style={column name=\textbf{Uitdagingen}, column type={l}},  
  every head row/.style={
    before row=\toprule,
    after row=\midrule},
  every last row/.style={
    after row=\bottomrule}
]{tabellen/uitdagingen.csv}
%\caption{Uitdagingen}
%\label{tabel:uitdagingen}
%\end{table}

In totaal konden 13 uitdagingen en 38 onderuitdagingen uit de POC geëxtraheerd worden.


De wijze waarop het raamwerk de uitdaging aangaat zal tot een score voor de uitdaging leiden.
Er onderscheiden zich drie gevallen.
De hoogste score ($2$) wordt toegekend wanneer bepaalde functionaliteit reeds aangeboden wordt door het raamwerk. 
Een lagere score ($1$) betekent dat een plug-in werd gezocht.
Omdat de raamwerken verderbouwen op HTML5, zal een kenmerk van HTML5 ook als plug-in beschouwd worden.
Voor een oplijsting van de HTML5 kenmerken wordt naar sectie \ref{sec:html5-css3-js} verwezen.
Wanneer de implementatie zelf diende geschreven te worden of een hack noodzakelijk was, zal de laagste score ($0$) worden toegekend.
Ook is het mogelijk  dat de uitdaging helemaal niet werd geïmplementeerd.
Dit is mogelijk wanneer het raamwerk de functionaliteit niet ondersteund,  geen plug-in werd gevonden en niet aan een eigen implementatie werd begonnen.
Dit zal leiden tot een $0$ score.
Wanneer CSS-code werd gebruikt om de uitdaging op te lossen zal de laagste score worden toegekend.
Het gebruik van CSS3 wordt echter als kenmerk van HTML5 gezien en vervolgens met $1$ gequoteerd.

Tabel \ref{tabel:scores-uitdagingen} toont de mogelijke scores $U_{r,i}$ van raamwerk $r$ en voor uitdaging $i$.
\begin{table}[h]	
  \centering
  \begin{tabular}{ll}
    \toprule
    \textbf{Score} & \textbf{Verklaring}\\
    \midrule
    $U_{r,i} = 2$ & Ondersteund door het raamwerk\\
    $U_{r,i} = 1$ & Een plug-in is nodig of nieuwe kenmerken van HTML5\\
    $U_{r,i} = 0$ & Eigen implementatie of hack of niet geïmplementeerd\\ 
    \bottomrule
  \end{tabular}
  \caption{Beoordeling uitdagingen gebruikscriterium}
  \label{tabel:scores-uitdagingen}
\end{table}

De potentiele score van een uitdaging is discreet en ligt tussen $0$ en $2$.
Er zijn dus slechts $3$ scores waaruit gekozen kan worden om de implementatie te beoordelen.
De verklaringen bij de scores omvatten alle gevallen op een eenduidige manier.
Een alternatief bestond uit $4$ scores waarbij HTML5 kenmerken een lagere score kregen in vergelijking met plug-ins.
Omdat de raamwerken afhankelijk zijn van HTML5 werd hiervoor niet gekozen.
%TODO beter uitleggen?

\begin{equation}
  \text{Gebruik}_r = \sum_{i=1}^{38}{\left(U_{r,i}\right)}
  \label{eq:gebruik}
\end{equation}
met $r$ de verschillende raamwerken en $i$ de uitdagingen.

Omdat er $38$ onderuitdagingen zijn kan het raamwerk voor dit criterium maximaal $76$ behalen.

Alle interessante functionaliteit die in de POC werd gebruikt, zit in een uitdaging vervat.  
Interessante functionaliteit slaat op functionaliteit die potentieel door het raamwerk kan worden ondersteund.
De POC bevat bijvoorbeeld ook een omzetting van \term{identifiers} naar hun tekstuele vorm.
Dit is geen interessante functionaliteit omdat het eigen is aan de POC zelf.
De implementatie hiervan zal uitsluitend uit \term{native} \js{}-code bestaan.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Ondersteuning}
\label{sec:vergelijking-ondersteuning}
Dit criterium moet weergeven hoe goed het raamwerk verschillende toestellen en verschillende besturingssystemen ondersteund.
Enkel de standaard browser van het besturingssysteem zal beschouwd worden.
Voor Android toestellen is dit de Android browser of Chrome.  
Er geldt dat vanaf versie 4.0 Chrome als standaard browser wordt gezien~\cite{Wimberly2008}.
Voor iOS is Safari de standaard browser


Een context wordt gedefinieerd als één bepaalde configuratie van toestel, besturingssysteem en browser.
In elke context zal de functionaliteit van de POC op ondersteuning worden getest met uitdagingen.
Uitdagingen die gebruikt zijn om het gebruikscriterium te testen, kunnen hier worden hergebruikt.
Aangezien sommige uitdagingen triviaal gelden voor elk apparaat zal er slechts een subset van deze uitdagingen getest worden.
Triviale uitdagingen zijn uitdagingen waarbij de implementatie in elk raamwerk enkel uit \js bestaat.
Deze uitdagingen zijn:
\begin{itemize}
 \item \uit{anatomie}
 \item \uit{laadscherm}
 \item \uit{vullen}
 \item \uit{ajax}
 \item \uit{lijsten}
\end{itemize}
Er resten dan nog $8$ uitdagingen.
Voor dit criterium worden alle onderuitdagingen verwaarloosd behalve bij \uit{formulieren} en \uit{offline}.
%De onderuitdagingen zijn te specifiek om op ondersteuning te testen.
Een uitdaging zal enkel slagen als alle onderuitdagingen ondersteund worden.
Zo kan bijvoorbeeld op een apparaat getest worden of auto-aanvullen werkt.
Uitdaging \uit{autoaanvullen} bevat als onderuitdagingen het ophalen van suggesties en het tonen van een dropdownmenu.
De werking van de uitdaging is een combinatie van beide en zal dus enkel slagen als beide worden ondersteund.
De onderuitdagingen van \uit{formulieren} en \uit{offline} kunnen wel op ondersteuning worden getest.
De twee deeluitdagingen van \term{datepicker} die bij \uit{formulieren} horen zullen wel worden samengenomen zodat enkel een \term{datepicker} op zich en niet een aanpasbare \term{datepicker} op ondersteuning wordt gecontroleerd.


Het is belangrijk dat het raamwerk en niet een eigen implementatie op ondersteuning wordt getest.
Wanneer een uitdaging in het vorige criterium een $0$ behaalde, wil dit zeggen dat het raamwerk de uitdaging al niet ondersteunde.
In dit geval moet er niet op ondersteuning getest worden.
Hierdoor is het aantal uitdagingen of onderuitdagingen die getest worden afhankelijk van het raamwerk.


Deze score van een uitdaging of onderuitdaging kan $1$ of $0$ zijn.
Dit slaat op respectievelijk een vlekkeloze of problematieke uitvoering.
De maximale score die het raamwerk per context kan behalen is $13$.
In totaal zullen $8$ contexten worden gebruikt.
Deze worden in tabel \ref{tabel:toestellen-hci} weergegeven.

 \begin{table}[H]
 \centering
 \resizebox{14cm}{!} {
 \pgfplotstabletypeset[
   begin table=\begin{tabular}{l l l l l},
   end table=\end{tabular},
   col sep=comma,
   header=true,
   string type,
   skip coltypes=true,
   columns={Apparaat,Soort,Lancering,BS,Browser},
   columns/Apparaat/.style={column name=\textbf{Apparaat}},  
   columns/Soort/.style={column name=\textbf{Soort}},
   columns/Lancering/.style={column name=\textbf{Lancering}},
   columns/BS/.style={column name=\textbf{BS}},
   columns/Browser/.style={column name=\textbf{Browser}},
   every head row/.style={
     before row=\toprule,
     after row=\midrule},
   every last row/.style={
     after row=\bottomrule}
 ]{tabellen/apparaten.csv}
 }
 \caption{Beschikbare apparaten met hun besturingssysteem~(BS) en browser.}
 \label{tabel:toestellen-hci}
 \end{table}
 
De keuze van de $8$ contexten waarop ondersteuning wordt getest, is voornamelijk bepaald door het beschikbaarheid van de apparaten op het departement computerwetenschappen van de KU Leuven.
Er werd een evenwichtige keuze gemaakt tussen besturingssysteem,  browser en type apparaat.
Er werd gekozen voor $4$ Android en $4$ iOS apparaten.
Bij de $4$ Android apparaten zijn er $2$ met een Android browser en $2$ met Chrome browser.
Ook werd er gekozen voor $4$ smartphones en $4$ tablets.

De som van de scores van de verschillende contexten bepaalt de score van het ondersteuningscriterium:
\begin{equation}
  \text{Ondersteuning}_r = \sum_{c=1}^{8}{\left(\sum_{i=1}^{N_f}U_{r,c,i}\right)}
  \label{eq:ondersteuning}
\end{equation}
met $r$ de verschillende raamwerken,  $N$ het aantal geïmplementeerd uitdagingen en onderuitdagingen,  $c$ de verschillende contexten en $i$ de uitdagingen. 


Indien het raamwerk een implementatie bevat voor alle uitdagingen en onderuitdagingen kan er per context maximaal $13$ gescoord worden.
Indien alle $8$ contexten alle uitdagingen en onderuitdagingen correct weergeven zal de maximale score van $104$ behaald worden.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Performantie}
\label{sec:vergelijking-performantie}
Dit criterium zal de laattijden van het raamwerk in rekening brengen.

Performantie kan worden opgesplitst in twee verschillende factoren: opstarten en gebruiken van de applicatie.
De eerste factor zal met laadtijden worden aangeduid,  de tweede met gebruikservaring.

Bij de laadtijden onderscheiden zich vier gevallen die samen de totale laadtijd bepalen.
Ten eerste zal de laadtijd van de volledig geïmplementeerde POC bekeken worden ($l_{r,c,POC}$). 
Vervolgens zal de tijd worden opgemeten om de POC uit het cachegeheugen te laden ($l_{r,c,POC_{cache}}$).
Dezelfde twee gevallen zullen ook voor de loginapplicatie die in sectie \ref{sec:vergelijking-productiviteit} werden geïntroduceerd, worden opgemeten ($l_{r,c,login}$ en $l_{r,c,login_{cache}}$).
De redenen waarom deze applicatie wordt gebruikt, is omdat ervan kan worden uitgegaan dat niet de volledige POC in ieder raamwerk zal kunnen worden geïmplementeerd. 
Dit is in tegenstelling tot deze applicatie die gelijkaardig is in de vier raamwerken.

Het opmeten van laadtijden zal met PCAP Web Performance Analyzer gebeuren~\cite{SongL.bmcquadeMdsteele2010}.
Deze tool kan HAR-bestanden (HTTP \term{Archive}) analyseren.
Dit zijn bestanden die sporen van HTTP-trafiek bevatten.
Om deze bestanden te verkrijgen moet een laptop als hotspot worden ingesteld en de apparaten connecteren op de hotspot.
Wireshark of TCPdump kunnen op de hotspot HTTP-verkeer opmerken en bijhorende HAR-bestanden aanmaken.

Elke meting zal drie keer worden uitgevoerd. %TODO moeten deze drie metingen ook in de formule worden opgenomen?
Het gemiddelde van de drie metingen zal de uiteindelijke laadtijd bepalen. %TODO referentie 3 metingen ?
% Tim: de redenen was dat het manueel werk is, dus nu zaten we voor 3 metingen op 8 devices aan ongeveer 1 werkdag om dat uit te voeren


De gebruikservaring zal gemeten worden door de tijd op te meten hoelang het raamwerk nodig heeft om GI-elementen te renderen.
Hiervoor zal de lijst van $850$ elementen worden gebruikt die getoond wordt na aanmelden op de loginapplicatie.
De tijd die het raamwerk nodig heeft om de lijst de renderen ($l_{r,c,lijst}$) kan gemeten worden in \js.

De laadtijden en gebruikservaring zullen op acht verschillende apparaten worden opgemeten.
De gemiddelde laadtijd zal bekomen worden door de totale laadtijd door het aantal toestellen ($8$) te delen.
Dit zijn dezelfde als bij het ondersteuningscriterium (zie tabel \ref{tabel:toestellen-hci}).

De formule voor het performantiecriterium wordt dan:
%TODO: ik heb nu 2 tijdelijke formules gemaakt, omdat ik die kan hergebruiken in evaluatie 
\begin{equation}
  \text{Totale downloadtijd}_r= \sum_{c=1}^{8}{\left(l_{r,c,POC}+l_{r,c,POC_{cache}}+l_{r,c,login}+l_{r,c,login_{cache}}\right)}
    \label{eq:totale-downloadtijd}
\end{equation}
\begin{equation}
  \text{Totale gebruikerservaring}_r= \sum_{c=1}^{8}{\left(l_{r,c,lijst}\right)}
  \label{eq:totale-gebruikerservaring}
\end{equation}
\begin{equation}
  \text{Performantie}_r= \frac{\text{Totale downloadtijd}_r + \text{Totale gebruikerservaring}_r}{8}
  \label{eq:performantie}
\end{equation}
met $c$ de verschillende contexten en $r$ de verschillende raamwerken.

De score komt dus overeen met de responstijd van de applicatie.
%TODO: PE toch klopt niet, we tellen 2 tijden op (zowel download als rendertijd) en daarop gaan we dan die formule van Nielsen op loslaten, ik denk dat je gewoon die Nielson op de 2 apparaten tijden kunt loslaten, maar niet op de som van de 2..
Dit kan gecategoriseerd worden met limieten zoals opgelegd van Jakob Nielsen~\cite{Nielsen1993}:  $PE_r < 0.1$,  $PE_r < 1$ en $PE_r < 10$ (Alle tijden worden in seconden uitgedrukt).
De eerste twee categorieën zijn aanvaardbaar.

Om de scores van het performantiecriterium te staven zullen twee eigenschappen van de code bekeken worden:  de downloadgrootte en het aantal lijnen HTML-code.
Ook zullen de resultaten gecontroleerd worden met twee andere testen.
Een eerste extra test is Google Page Speed~\cite{Morgan2011}. 
Deze tool kan de code van een webapplicatie analyseren en testen op performantie specifiek voor mobiele apparaten.
Het resultaat is een score op 100 en een lijst van werkpunten om de performantie van de applicatie te verbeteren.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Spinnenweb}
\label{sec:vergelijking-spinnenweb}

Om de scores van de vijf criteria in een spinnenweb te kunnen plotten moet elke score op dezelfde schaal worden gebracht om duidelijk de verschillen te kunnen waarnemen.
De gebruikte MatLab extensie die spinnenwebben genereert, vereist dit ook~\cite{Martti2007}.
Er werd gekozen om alle scores te relativeren.
Hiervoor moet elke score van een criterium gedeeld worden door het maximaal behaalde resultaat van dat criterium.
Alle scores zullen vervolgens tussen $0$ en $1$ liggen.
Deze methode zal ervoor zorgen dat het raamwerk met de beste score een $1$ behaalt.

Om verwarring te voorkomen, moeten ook de scores voor het productiviteitscriterium en performantiecriterium geinverteerd worden.
Dit komt omdat voor deze critera geldt:  hoe lager de score,  hoe beter het raamwerk.

De formules om de relatieve scores te bereken worden hieronder weergegeven.
De relatieve scores zullen gebruikt worden om het spinneweb op te stellen.

\begin{equation}
  REL(\text{Populariteit}_r)=\frac{\text{Populariteit}_r}{\max_{m}\{\text{Populariteit}_m\}}
  \label{eq:rel-populariteit}
\end{equation}

\begin{equation}
  REL(\text{Productiviteit}_r) = \frac{\left(\text{Productiviteit}_R\right)^{-1}}{\max_{m}\{\left(\text{Productiviteit}_m\right)^{-1}\}}
  \label{eq:rel-productiviteit}
\end{equation}

\begin{equation}
  REL(\text{Gebruik}_r) = \frac{\text{Gebruik}_R}{\max_{m}\{\text{Gebruik}_m\}}
  \label{eq:rel-gebruik}
\end{equation}

\begin{equation}
  REL(\text{Ondersteuning}_r) = \frac{\text{Ondersteuning}_r}{\max_{m}\{\text{Ondersteuning}_m\}}
  \label{eq:rel-ondersteuning}
\end{equation}

\begin{equation}
  REL(\text{Performantie}_r)= \frac{\left(\text{Performantie}_r\right)^{-1}}{\max_{m}\{\left(\text{Performantie}_m\right)^{-1}\}}
  \label{eq:rel-performantie}
\end{equation}
